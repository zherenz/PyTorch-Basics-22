{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zherenz/PyTorch-Basics-22/blob/main/torch_basics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x76JKTCA1bMy"
      },
      "source": [
        "# Dataset & DataLoader<br>\n",
        "torch.utils.data.Dataset<br>\n",
        "torch.utils.data.Dataset (Segmentation)<br>\n",
        "torchvision.datasets<br>\n",
        "torchvision.datasets.ImageFolder<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZNkqveTuS8gQ"
      },
      "source": [
        "## torch.utils.data.Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "XTtjljDvTFMz"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(tensor([0.8207, 1.0084]), 1)\n"
          ]
        }
      ],
      "source": [
        "# a binary classification\n",
        "# txt file: [0]height [1]weight [2]gender\n",
        "\n",
        "import torch\n",
        "\n",
        "class GenderDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, txt):\n",
        "        with open(txt) as f:\n",
        "            lines = f.readlines()\n",
        "        data = []\n",
        "        for line in lines:\n",
        "            line = line.strip('\\n')\n",
        "            words = line.split()\n",
        "            data.append((float(words[0]) / 2.0, float(words[1]) / 80.0, int(words[2])))\n",
        "        self.data = data\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return torch.tensor([self.data[index][0], self.data[index][1]]), self.data[index][2]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "train_data = GenderDataset(txt='data_gender_train.txt')\n",
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size=5, shuffle=True)\n",
        "\n",
        "print(train_data[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Load image type data from CSV\n",
        "\n",
        "df = pandas.read_csv(\"xxx.csv\") <br>\n",
        "datas0 = datafile.iloc[:,0].values &emsp; shape:(10000, 784)<br>\n",
        "datas1 = datafile.iloc[:,1:].values &emsp; shape: (10000, )<br>\n",
        "<br>\n",
        "ToTensor: Converts a PIL Image or numpy.ndarray (H x W x C) in the range [0, 255] <br>\n",
        "to a torch.FloatTensor of shape (C x H x W) in the range [0.0, 1.0] <br>\n",
        "\n",
        "**DataLoader: (batch_size, channel, H, W) --> torch.Size([2, 1, 28, 28])**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([2, 1, 28, 28])\n"
          ]
        }
      ],
      "source": [
        "# FashionMNIST\n",
        "# https://www.kaggle.com/zalando-research/fashionmnist\n",
        "import torch, pandas\n",
        "import numpy as np\n",
        "import torchvision.transforms as ttf\n",
        "\n",
        "class FMDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, datafile, transform=None):\n",
        "        self.transform = transform\n",
        "        self.images = datafile.iloc[:,1:].values.astype(np.uint8) # from 0-255\n",
        "        # print(self.images.shape) # (10000, 784)\n",
        "        self.labels = datafile.iloc[:, 0].values\n",
        "        # print(self.labels.shape) # (10000,)\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        # len(self.images[idx]) = 784\n",
        "        image = self.images[idx]\n",
        "        label = int(self.labels[idx])\n",
        "        if self.transform is not None:\n",
        "            # numpy image: H x W x C\n",
        "            image = image.reshape(28, 28, 1)\n",
        "            # ToTensor: Converts a PIL Image or numpy.ndarray (H x W x C) in the range [0, 255] \n",
        "            # to a torch.FloatTensor of shape (C x H x W) in the range [0.0, 1.0] \n",
        "            image = self.transform(image)\n",
        "        else:\n",
        "            # torch image: C x H x W\n",
        "            image = image.reshape(1, 28, 28)\n",
        "            image = torch.tensor(image/255., dtype=torch.float)\n",
        "        label = torch.tensor(label, dtype=torch.long)\n",
        "        return image, label\n",
        "\n",
        "# csv {label: 1, pixel1: 0, pixel2: 0, ...}\n",
        "test_df = pandas.read_csv(\"fashion-mnist_test.csv\")\n",
        "test_data = FMDataset(test_df, ttf.ToTensor())\n",
        "test_loader = torch.utils.data.DataLoader(test_data, batch_size=2, shuffle=False)\n",
        "\n",
        "print(next(iter(test_loader))[0].shape)\n",
        "# torch.Size([2, 1, 28, 28])\n",
        "# (batch_size, channel, H, W)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dataset (Segmentation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch, cv2\n",
        "\n",
        "class SegmentationDataset(torch.utils.data.Dataset):\n",
        "\tdef __init__(self, imagePaths, maskPaths, transforms):\n",
        "\t\t# store the image and mask filepaths, and augmentation\n",
        "\t\t# transforms\n",
        "\t\tself.imagePaths = imagePaths\n",
        "\t\tself.maskPaths = maskPaths\n",
        "\t\tself.transforms = transforms\n",
        "  \n",
        "\tdef __len__(self):\n",
        "\t\treturn len(self.imagePaths)\n",
        "\n",
        "\tdef __getitem__(self, idx):\n",
        "\t\t# load the image from disk, swap its channels from BGR to RGB,\n",
        "\t\t# and read the associated mask from disk in grayscale mode\n",
        "\t\timage = cv2.imread(self.imagePaths[idx])\n",
        "\t\timage = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\t\tmask = cv2.imread(self.maskPaths[idx], 0)\n",
        "\n",
        "\t\tif self.transforms is not None:\n",
        "\t\t\timage = self.transforms(image)\n",
        "\t\t\tmask = self.transforms(mask)\n",
        "\n",
        "\t\treturn (image, mask)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D1s15GIygym_"
      },
      "source": [
        "## torchvision.datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Xi9rrnF1Sz2"
      },
      "outputs": [],
      "source": [
        "from torchvision import datasets\n",
        "import torchvision\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision.transforms as ttf\n",
        "\n",
        "# torchvision.datasets.FashionMNIST\n",
        "train_data = datasets.FashionMNIST(root=\"data\", train=True, download=False, transform=ttf.ToTensor())\n",
        "test_data = datasets.FashionMNIST(root=\"data\", train=False, download=False, transform=ttf.ToTensor())\n",
        "\n",
        "train_loader = DataLoader(train_data, batch_size=128, shuffle=True, num_workers=2, drop_last=True)\n",
        "test_loader = DataLoader(test_data, batch_size=128, shuffle=False, num_workers=2)\n",
        "\n",
        "# torchvision.datasets.CIFAR10\n",
        "train_set = torchvision.datasets.CIFAR10(root='/content/drive/MyDrive/deep_learning/data', train=True, download=False, transform=ttf.ToTensor())\n",
        "train_loader = torch.utils.data.DataLoader(train_set, batch_size=128, shuffle=True, num_workers=2)\n",
        "\n",
        "\n",
        "# torchvision.datasets.ImageNet(root: str, split: str = 'train', **kwargs: Any)\n",
        "imagenet_data = torchvision.datasets.ImageNet('path/to/imagenet_root/')\n",
        "data_loader = torch.utils.data.DataLoader(imagenet_data,\n",
        "                                          batch_size=4,\n",
        "                                          shuffle=True,\n",
        "                                          num_workers=2)\n",
        "\n",
        "# We have loaded that dataset into the DataLoader and can iterate through the dataset as needed. \n",
        "# Each iteration below returns a batch of train_features and train_labels (containing batch_size=64 features and labels respectively)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Mj1K9VJ-LP5"
      },
      "source": [
        "## torchvision.datasets.ImageFolder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bTfqD5f6asTX"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as ttf\n",
        "\n",
        "train_data = torchvision.datasets.ImageFolder(train_path, transform=ttf.ToTensor())\n",
        "val_data = torchvision.datasets.ImageFolder(val_path, transform=ttf.ToTensor())\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size=128, num_workers=4, shuffle=True)\n",
        "val_loader = torch.utils.data.DataLoader(val_data, batch_size=128, num_worker=4, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Hm8tnYy3e9f"
      },
      "outputs": [],
      "source": [
        "train_transforms = [ttf.ToTensor(), ttf.RandomHorizontalFlip(), ttf.RandomAffine(degrees=(-15, 15), scale=(0.98, 1.03)), ttf.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))]\n",
        "val_transforms = [ttf.ToTensor(), ttf.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))]\n",
        "\n",
        "# root/dog/xxx.png\n",
        "# root/dog/xxy.png\n",
        "# root/dog/[...]/xxz.png\n",
        "\n",
        "# root/cat/123.png\n",
        "# root/cat/nsdf3.png\n",
        "# root/cat/[...]/asd932_.png"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T2INiQVNhnaU"
      },
      "source": [
        "# Model<br>\n",
        "torch.nn.Module<br>\n",
        "torch.nn.Sequential<br>\n",
        "torchvision.models.resnet18<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_z00VQ5mkxiy"
      },
      "source": [
        "## torch.nn.Module (LeNet) <br>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U4TPpn2Gh1fK"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Model(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
        "        x = F.max_pool2d(F.relu(self.conv2(x)), (2, 2))\n",
        "        x = torch.flatten(self.num_flat_features(x), start_dim=1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "    def num_flat_features(self, x):\n",
        "        size = x.size()[1:]\n",
        "        num_features = 1\n",
        "        for s in size:\n",
        "            num_features *= s\n",
        "        return num_features\n",
        "\n",
        "\n",
        "model = Model()\n",
        "model = model.cuda()\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JERW7tfrlpLg"
      },
      "source": [
        "## torch.nn.Sequential"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bcS4slZ-lvWy"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "model = nn.Sequential(\n",
        "    nn.Conv2d(3, 32, 3),\n",
        "    nn.BatchNorm2d(32),\n",
        "    nn.ReLU(),\n",
        "    nn.MaxPool2d(2,2),\n",
        "    \n",
        "    nn.Conv2d(32, 64, 3),\n",
        "    nn.BatchNorm2d(64),\n",
        "    nn.ReLU(),\n",
        "    nn.MaxPool2d(2,2),\n",
        "    # print the size of the output --> output.size() / output.shape\n",
        "    # to find out the input size of fc \n",
        "    \n",
        "    # flatten from 2nd dimesion (batch)\n",
        "    nn.Flatten(),\n",
        "    nn.Linear(64 * 6 * 6, 4096),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(0.2),\n",
        "    nn.Linear(4096, 10)\n",
        ")\n",
        "\n",
        "model = model.cuda()\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## torchvision.models.resnet18 <br>\n",
        "model = torchvision.models.resnet18() <br>\n",
        "model.fc = nn.Linear(512, out_feats) <br>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torchvision import models\n",
        "import torch.nn as nn\n",
        "\n",
        "model = models.resnet18(pretrained=False)\n",
        "# print(model)\n",
        "\n",
        "# modify input channels\n",
        "model.conv1 = nn.Conv2d(1, 64, kernel_size=(7,7), stride=(2,2), padding=(3,3), bias=False)\n",
        "\n",
        "# (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
        "# model.fc = nn.Linear(512, out_features)\n",
        "model.fc = nn.Linear(512, 2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iGSZcFQWmo2A"
      },
      "source": [
        "# Loss Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IEs8Aul6nQ3j"
      },
      "outputs": [],
      "source": [
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "criterion = torch.nn.BCELoss(weight=None, size_average=None, reduce=None, reduction='mean')\n",
        "criterion = torch.nn.L1Loss(size_average=None, reduce=None, reduction='mean')\n",
        "criterion = torch.nn.MSELoss(size_average=None, reduce=None, reduction='mean')\n",
        "criterion = torch.nn.SmoothL1Loss(size_average=None, reduce=None, reduction='mean', beta=1.0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gMpS7RoQaKD5"
      },
      "source": [
        "## BCE-Dice Loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fkwX8HdbaHXD"
      },
      "outputs": [],
      "source": [
        "class DiceBCELoss(nn.Module):\n",
        "    def __init__(self, weight=None, size_average=True):\n",
        "        super(DiceBCELoss, self).__init__()\n",
        "\n",
        "    def forward(self, inputs, targets, smooth=1):\n",
        "        inputs = F.sigmoid(inputs)       \n",
        "        inputs = inputs.view(-1)\n",
        "        targets = targets.view(-1)\n",
        "        intersection = (inputs * targets).sum()                     \n",
        "        dice_loss = 1 - (2.*intersection + smooth)/(inputs.sum() + targets.sum() + smooth)  \n",
        "        BCE = F.binary_cross_entropy(inputs, targets, reduction='mean')\n",
        "        Dice_BCE = BCE + dice_loss\n",
        "        \n",
        "        return Dice_BCE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## CrossEntrophy Loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The CrossEntropy function, in PyTorch, expects the output from your model to be of the shape - **[batch, num_classes, H, W]** (pass this directly to your loss function)<br>\n",
        "and the ground truth to be of shape **[batch, H, W]** where H, W in your case is 256, 256.<br>\n",
        "Also please make sure the ground truth is of type long by calling .long() on the tensor<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tbnUK7lroB1M"
      },
      "source": [
        "# Train & Eval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hllh2Q8WoGVl"
      },
      "outputs": [],
      "source": [
        "def train():\n",
        "    \n",
        "    # model = Model()\n",
        "    # model = model.cuda()\n",
        "    # train_data = MyDataset()\n",
        "    # train_loader = DataLoader(train_data, 128)\n",
        "    \n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, 0.98)\n",
        "    epochs = 5\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        for data, label in train_loader:\n",
        "            data, label = data.cuda(), label.cuda()\n",
        "            optimizer.zero_grad()\n",
        "            output = model(data)\n",
        "            loss = criterion(label, output)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += float(loss)  # or loss.item() detached from cuda and tensor\n",
        "        \n",
        "        scheduler.step()\n",
        "        train_loss = total_loss / len(train_loader)\n",
        "        print('Epoch: {} \\tTraining Loss: {:.6f}'.format(epoch, train_loss))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jEqeb4XoumpN"
      },
      "outputs": [],
      "source": [
        "def val(epoch):       \n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    num_correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, label in val_loader:\n",
        "            data, label = data.cuda(), label.cuda()\n",
        "            output = model(data)\n",
        "            loss = criterion(output, label)\n",
        "            total_loss += loss.item()\n",
        "            preds = torch.argmax(output, 1)\n",
        "            num_correct += int((preds == label.data).sum())\n",
        "    \n",
        "    val_loss = total_loss / len(val_loader)\n",
        "    acc = 100 * num_correct / len(val_set)\n",
        "    print('Epoch: {}  Val Loss: {}  Acc: {}'.format(epoch, val_loss, acc))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zSY9MFBLxSSt"
      },
      "source": [
        "# Optimizer & Scheduler\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mbKyPOwXxWa0"
      },
      "outputs": [],
      "source": [
        "# torch.optim.ASGD\n",
        "# torch.optim.Adadelta\n",
        "# torch.optim.Adagrad\n",
        "# torch.optim.Adam\n",
        "# torch.optim.AdamW\n",
        "# torch.optim.Adamax\n",
        "# torch.optim.LBFGS\n",
        "# torch.optim.RMSprop\n",
        "# torch.optim.Rprop\n",
        "# torch.optim.SGD\n",
        "# torch.optim.SparseAdam\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.98)\n",
        "# scheduler.step()\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=2.0e-3, weight_decay=5e-6)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.7, patience=2)\n",
        "# scheduler.step(val_loss)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyNJjHuq8tvX3DMDg4qLfAHP",
      "collapsed_sections": [
        "x76JKTCA1bMy",
        "D1s15GIygym_",
        "_Mj1K9VJ-LP5",
        "T2INiQVNhnaU",
        "iGSZcFQWmo2A"
      ],
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.5 ('torch')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    },
    "vscode": {
      "interpreter": {
        "hash": "bf27a1f4fd1180422e52c90b61df446e73053a3ba274c1f578fcd274652af2b8"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
